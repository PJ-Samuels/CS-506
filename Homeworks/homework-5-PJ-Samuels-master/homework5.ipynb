{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 (100 Points)\n",
    "\n",
    "## Exercise 1 (55 pts)\n",
    "\n",
    "For this exercise we’ll be working with two years of the bicycle sharing systems for New York City (Citibike). The dataset contains daily bike trip counts, along with daily measurements on environmental and seasonal information that may affect the bikesharing.\n",
    "\n",
    "Here’s information on what the variables mean.\n",
    "\n",
    "- trips - daily total number of bike trips taken (all stations)\n",
    "- precipitation - daily inches of rain\n",
    "- snow_depth - daily inches of snow. Accoding to NOAA \"Determine the depth of the new and old snow remaining on the ground at observation time\".\n",
    "- snowfall - according to NOAA \"Measure and record the snowfall (snow, ice pellets ) since the previous snowfall observation (24 hours).\"\n",
    "- max_temperature - daily maximum temperature in Farenheit (highest temperature reached)\n",
    "- min_temperature - daily minimum temperature in Farenheit (lowest temperature reached)\n",
    "- average_wind_speed - measured hourly in mph and averaged for daily value\n",
    "- year \n",
    "- holiday\n",
    "    - True\n",
    "    - False\n",
    "- stations in service - docking stations working per day\n",
    "- weekday\n",
    "    - True\n",
    "    - False\n",
    "- weekday_non_holiday\n",
    "    - True\n",
    "    - False\n",
    "\n",
    "More details on snow measurements [here](https://www.weather.gov/gsp/snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the libraries that we will need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import plotly.express as px\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data pre-processing (7 pts)\n",
    "\n",
    "Let's start by loading and pre-processing our dataset.\n",
    "\n",
    "a) Load the `bikes` dataset into a dataframe called `bikes`. Take care of any missing values appropriately. [2pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Separate your dataset into two dataframe. One dataframe should contain the data for weekends and the other one for weekdays [2pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Let's look at the number of bicycle rental (trips) per season! Draw a boxplot that displays this information for the whole dataset. As subplots, draw the boxplots for weekdays and weekends. Is there a differece in the distribution of trips between seasons? Is there a difference between weekdays and weekends? Explain. [3pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Qualitative predictors (10 pts)\n",
    "\n",
    "The Season variable is an example of what’s called a categorical predictor. In this part you will fit a model with a qualitative predictor and interpret the findings.\n",
    "\n",
    "a) Using the weekdays dataset, fit a linear regression model with trips as the response variable and season as your predictor, and print the summary of this linear regression model. [2pt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) How many total coefficients are there in the model? What does each coefficient correspond to? [1pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Interpret the coefficients of season in the model. Make sure to use an example in your interpretation, related to the bikes [2pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Repeat a) -> c) with your weekends dataset. What difference can you see? [5pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Multiple linear regression (10 pts)\n",
    "\n",
    "In this problem we’ll practice fitting and interpreting the results of a multiple linear regression. For this question use the weekdays dataset.\n",
    "\n",
    "a) Fit a regression model with trips as the response variable and the following predictors as features: snow_depth, snowfall, min_temperature, max_temperature, precipitation, month [2pt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Interpret the coefficients of snow_depth, snowfall, min_temperature, max_temperature, precipitation, month in the model [2pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Re-run the model from 3)a) but add another predictor of your choice. Justify your choice of predictor. Did this make any difference? Did the R-squared of the model improve? [2pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) From c) which predictors are associated with increased ridership? Which predictors are associated with decreased ridership? [2pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Which predictors are statistically significant (95% confidence level)? [2pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Dealing with collinearity (8 pts)\n",
    "\n",
    "Highly correlated predictors can make interpreting regression coefficients problematic (they do not contribute relevant information for the model). Let’s evaluate this in our dataset. Continue using weekdays as your main dataset.\n",
    "\n",
    "a) Check the variables used in Problem 3)a) to see if any of the predictors are highly correlated with one another. (Lab 7 may help you here) [2pt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Are any of the predictors highly correlated? Are you surprised that these predictors are highly correlated, or can you think of a reason for why it makes sense that they should be correlated? [2pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Fit your regression model, but this time omit the max or min temperature variable. Display the coefficients table for this model. [2pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) What is the coefficient of temperature in this new model? Is it very different from the temperature coefficient estimated in part 3)a)? Is it statistically significant? Explain your findings. [2pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# You can add more cells if you need them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Exploring different models (20 pts)\n",
    "\n",
    "*Continue using weekdays as your dataset.*\n",
    "\n",
    "a) Construct a scatterplot of trips ~ month. Describe what you see. Does a linear relationship appear to be a good way of modeling how bikeshare count varies with month? [2pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Try out 3 different degrees of polynomial regression fits for modeling the relationship between trips and month. Do not do more than 3 models. Display all of them in the same plot (through subplots). Choose the subplot that appears to nicely capture the trends in the data. Be sure to print the $R^2$ or adjusted-$R^2$ for each subplot to help you with your decision. Explain your choice and include comments to explain your code. [4pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# You can add more cells if you need them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Repeat b) to determine appropriate degree polynomials for modeling the relationship between trip and the other inputs: temperature (pick one), snow (also pick one) and precipitation (i.e., trips ~ temperature, trips ~ snow, and trips ~precipitation). Justify your choices and include comment on your code. (Note: your polynomials can have different degrees for different features) [7pts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# You can add more cells if you need them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Repeat 5)c) with the weekends dataset. Do you notice any differences? Justify your choices and include comment on your code. [7pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# You can add more cells if you need them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (45pts)\n",
    "\n",
    "In this exercise we will implement a Natural Language Processing (NLP) system using binary logistic regression.\n",
    "\n",
    "The data you will be working with comes from the [Yelp Dataset](https://www.yelp.com/dataset). Each line is a review that consists of a label (0 for negative reviews and 1 for positive reviews) and a set of words.\n",
    "\n",
    "```\n",
    "1 i will never forget this single breakfast experience in mad...\n",
    "0 the search for decent chinese takeout in madison continues ...\n",
    "0 sorry but me julio fell way below the standard even for med...\n",
    "1 so this is the kind of food that will kill you so there s t...\n",
    "```\n",
    "\n",
    "In order to transform the set of words into vectors, we will rely on a method of feature engineering called word embeddings. Rather than simply indicating which words are present, word embeddings represent each word by \"embedding\" it in a low-dimensional vector space which may carry more information about the semantic meaning of the word. (for example in this space, the words \"King\" and \"Queen\" would be close).\n",
    "\n",
    "`word2vec.txt` contains the `word2vec` embeddings for about 15 thousand words. Not every word in each review is present in the provided `word2vec.txt` file. We can treat these words as being \"out of vocabulary\" and ignore them.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let x_i denote the sentence `“a hot dog is not a sandwich because it is not square”` and let a toy word2vec dictionary be as follows:\n",
    "\n",
    "```\n",
    "hot      0.1     0.2     0.3\n",
    "not      -0.1    0.2     -0.3\n",
    "sandwich 0.0     -0.2    0.4\n",
    "square   0.2     -0.1    0.5\n",
    "```\n",
    "\n",
    "we would first `trim` the sentence to only contain words in our vocabulary: `\"hot not sandwich not square”` then embed x_i into the feature space:\n",
    "\n",
    "$$ φ2(x_i)) = \\frac{1}{5} (word2vec(\\text{hot}) + 2 · word2vec(\\text{not}) + word2vec(\\text{sandwich}) + word2vec(\\text{square})) = \\left[0.02 \\hspace{2mm} 0.06 \\hspace{2mm} 0.12 \\hspace{2mm}\\right]^T $$\n",
    "\n",
    "### Part 1 (20pts)\n",
    "\n",
    "a) Implement a function to trim out-of-vocabulary words from the reviews. Your function should return an nd array of the same dimension and dtype as the original loaded dataset. (10pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "VECTOR_LEN = 300   # Length of word2vec vector\n",
    "MAX_WORD_LEN = 64  # Max word length in dict.txt and word2vec.txt\n",
    "\n",
    "################################################################################\n",
    "# We have provided you the functions for loading the tsv and txt files. Feel   #\n",
    "# free to use them! No need to change them at all.                             #\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def load_tsv_dataset(file):\n",
    "    \"\"\"\n",
    "    Loads raw data and returns a tuple containing the reviews and their ratings.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): File path to the dataset tsv file.\n",
    "\n",
    "    Returns:\n",
    "        An np.ndarray of shape N. N is the number of data points in the tsv file.\n",
    "        Each element dataset[i] is a tuple (label, review), where the label is\n",
    "        an integer (0 or 1) and the review is a string.\n",
    "    \"\"\"\n",
    "    dataset = np.loadtxt(file, delimiter='\\t', comments=None, encoding='utf-8',\n",
    "                         dtype='l,O')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_feature_dictionary(file):\n",
    "    \"\"\"\n",
    "    Creates a map of words to vectors using the file that has the word2vec\n",
    "    embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): File path to the word2vec embedding file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary indexed by words, returning the corresponding word2vec\n",
    "        embedding np.ndarray.\n",
    "    \"\"\"\n",
    "    word2vec_map = dict()\n",
    "    with open(file) as f:\n",
    "        read_file = csv.reader(f, delimiter='\\t')\n",
    "        for row in read_file:\n",
    "            word, embedding = row[0], row[1:]\n",
    "            word2vec_map[word] = np.array(embedding, dtype=float)\n",
    "    return word2vec_map\n",
    "\n",
    "\n",
    "def trim_reviews(path_to_dataset):\n",
    "    return\n",
    "\n",
    "trim_train = trim_reviews(\"./data/train_small.tsv\")\n",
    "trim_test = trim_reviews(\"./data/test_small.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Implement the embedding and store it to a `.tsv` file where the first column is the label and the rest are the features from the embedding. Round all numbers to 6 decimal places. `embedded_train_small.tsv` and `embedded_test_small.tsv` contain the expected output of your function. (10pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_reviews(trimmed_dataset):\n",
    "    return\n",
    "\n",
    "def save_as_tsv(dataset, filename):\n",
    "    with open(filename, 'w+') as f:\n",
    "        f.writelines(...)\n",
    "    return\n",
    "\n",
    "embedded_train = embed_reviews(trim_train)\n",
    "embedded_test = embed_reviews(trim_test)\n",
    "\n",
    "save_as_tsv(embedded_train, \"./data/output/embedded_train_small.tsv\")\n",
    "save_as_tsv(embedded_test, \"./data/output/embedded_test_small.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 (25pts)\n",
    "\n",
    "In this part we'll be implementing Stochastic Gradient Descent for binary Logistic Regression Classifier.\n",
    "\n",
    "Some rules:\n",
    "\n",
    "1. Include an intercept term in your model. You must consider the bias term as part of the weight vector and not a separate term to keep track of.\n",
    "2. Initialize all model parameters to 0\n",
    "3. Use vector and matrix multiplication\n",
    "4. Do not shuffle the data\n",
    "\n",
    "The expected `metrics.txt` from the dataset with `500` epochs and `0.001` learning rate is:\n",
    "\n",
    "```\n",
    "error(train): 0.000000\n",
    "error(test): 0.625000\n",
    "```\n",
    "\n",
    "We will be testing your code on other, larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_tsv_dataset(file):\n",
    "    return np.loadtxt(file, delimiter='\\t', encoding='utf-8')\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    e = np.exp(x)\n",
    "    return e / (1 + e)\n",
    "\n",
    "\n",
    "def sgd(theta, X, Y, learning_rate):\n",
    "    # TODO: implement in vector form\n",
    "    return theta\n",
    "\n",
    "\n",
    "def train(theta, X, y, num_epoch, learning_rate):\n",
    "    for _ in range(num_epoch):\n",
    "        theta = sgd(theta, X, y, learning_rate)\n",
    "    return theta\n",
    "\n",
    "\n",
    "def predict(theta, X):\n",
    "    # TODO: implement in vector form\n",
    "    return\n",
    "\n",
    "\n",
    "def compute_error(y_pred, y):\n",
    "    # TODO: implement in vector form\n",
    "    return\n",
    "\n",
    "\n",
    "def write_metrics(train_err, test_err, metrics_out):\n",
    "    with open(metrics_out, 'w+') as f:\n",
    "        w = \"error(train): \" + \"{:.6f}\".format(train_err) + \"\\n\"\n",
    "        w += \"error(test): \" + \"{:.6f}\".format(test_err) + \"\\n\"\n",
    "        f.write(w)\n",
    "    return\n",
    "\n",
    "\n",
    "def logistic_reg(formatted_train, formatted_test, metrics_out, num_epochs, learning_rate):\n",
    "    theta = ...\n",
    "    y = ...\n",
    "    X = ...\n",
    "\n",
    "    learned_theta = train(theta, X, y, num_epochs, learning_rate)\n",
    "    train_pred = predict(learned_theta, X)\n",
    "    train_err = compute_error(train_pred, y)\n",
    "\n",
    "    X_test = ...\n",
    "    y_test = ...\n",
    "    test_pred = predict(learned_theta, X_test)\n",
    "    test_err = compute_error(test_pred, y_test)\n",
    "\n",
    "    write_metrics(train_err, test_err, metrics_out)\n",
    "    return\n",
    "\n",
    "\n",
    "logistic_reg(\"./data/embedded_train_small.tsv\", \"./data/embedded_test_small.tsv\", \"./data/output/metrics.txt\", 500, 0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
